import pandas as pd

# File path to the large CSV file
file_path = '/Users/luyinyang/Downloads/Neuroscience/v1_biophysical/document/spikes1.csv'
v1_nodes = '/Users/luyinyang/Downloads/Neuroscience/v1_biophysical/network/v1_nodes.csv'

# Read the first 10000 rows
data1 = pd.read_csv(file_path,nrows=10000, sep=' ')
nodes = pd.read_csv(v1_nodes, sep=' ')

# Visualization code 1
import matplotlib.pyplot as plt
node_list=list(set(data1['node_ids']))
print(len(data1['node_ids']),len(node_list))
new_node_list=[node_list.index(i) for i in data1['node_ids']]

# add color list based on excitatory and inhibitory
color_list = []
for i in new_node_list:
    if nodes['ei'][node_list[i]] == 'e':
        color_list.append('r')
    else:
        color_list.append('b')

plt.scatter(x = data1['timestamps'], y = new_node_list, color=color_list ,s=8,linewidths=0,alpha=0.8)

# Add x and y labels
plt.xlabel('Timestamps (millisecond)')
plt.ylabel('Node ID')

plt.show()

# '''''''''''''''''''''''''''''''''
# Visualization code 2
import matplotlib.pyplot as plt
node_list=list(set(data1['node_ids']))
print(len(data1['node_ids']),len(node_list))
new_node_dict={i:node_list.index(i) for i in data1['node_ids']}
new_node_list=[new_node_dict[i] for i in data1['node_ids']]

color_dict=dict()
for i in new_node_list:
    if nodes['ei'][node_list[i]] == 'e':
        color_dict[i]='r'
    else:
        color_dict[i]='b'

color_list=[color_dict[i] for i in new_node_list]


plt.scatter(x = data1['timestamps'], y = new_node_list, color=color_list ,s=8,linewidths=0,alpha=0.8)

# Add x and y labels
plt.xlabel('Timestamps (millisecond)')
plt.ylabel('Node ID')

plt.show()

'''
# File path to the large CSV file
file_path = '/Users/luyinyang/Downloads/Neuroscience/v1_biophysical/output100/spikes100.csv'

# Read the first 100 rows
data100 = pd.read_csv(file_path, nrows=100)

# Display the first 100 rows
pd.set_option('display.max_rows', 100)
print(data100)
'''
'''
import h5py
with h5py.File('/Users/luyinyang/Downloads/Neuroscience/v1_biophysical/document/spikes1.h5',"r") as f:
    for key in f.keys():
        print(f[key], key, f[key].name)

spikes_group=f["spikes"]
for key in spikes_group.keys():
    print(spikes_group[key], key, spikes_group[key].name)
'''
'''
import h5py
import numpy as np

from sonata.reports.spike_trains import SpikeTrains, sort_order, PoissonSpikeGenerator

spikes = SpikeTrains.from_sonata('/Users/luyinyang/Downloads/Neuroscience/v1_biophysical/output100/spikes100.h5')
print(spikes.populations)

# Get the node_ids associated with the 'VTA' population
print(len(spikes.nodes('v1')))

# find the first and last spike time
print(spikes.time_range('v1'))

# get number of spikes generated by VTA neurons
print(spikes.n_spikes('v1'))
print(spikes.n_spikes('v1')/len(spikes.nodes('v1')))
# Returns all spikes as a dataframe
#spikes.to_dataframe()
'''